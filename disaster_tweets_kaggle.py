# -*- coding: utf-8 -*-
"""Disaster_Tweets_Kaggle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V1YBgEEGNm51ZFhMOULBG6qCOeBcpVns
"""

import re
import pandas as pd
from google.colab import files
uploaded = files.upload()

import numpy as np
import tensorflow as tf
import keras

import nltk
from nltk.stem.porter import *
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
nltk.download('all')

from nltk.tokenize.treebank import TreebankWordDetokenizer

data = pd.read_csv('train.csv')
data.head()

rel_data = data[['text', 'target']]
rel_data.head(20)

def clean_data(data):

    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    data = url_pattern.sub(r'', data)

    data = re.sub('\S*@\S*\s?', '', data)

    data = re.sub('\s+', ' ', data)

    data = re.sub("\'", "", data)

    #data = re.sub("\#", "", data)
        
    return data

temp = []
rel_data_to_list = rel_data['text'].values.tolist()
for i in rel_data_to_list:
  temp.append(clean_data(i))
temp[:20]

def lowerSent(sentence):
  '''
  :params:
  sentence: complete input string

  :returns:
  lower-cased input string
  '''
  return sentence.lower()

def tokenizeSent(sentence):
  '''
  :params:
  sentence: complete input string

  :returns:
  list of individual tokens
  '''
  tokenizer = RegexpTokenizer("[a-zA-Z@]+")
  return tokenizer.tokenize(sentence)

def removeStopWords(tokenizedSent):
  '''
  :params:
  tokenizedSent: list of individual tokens

  :returns:
  list of individual tokens w/o the stop words
  '''
  sw = set(stopwords.words('english'))
  return [w for w in tokenizedSent if w not in sw]
  
def stem(tokenizedSent):
  '''
  :params:
  tokenizedSent: list of individual tokens

  :returns:
  list of stemmed individual tokens
  '''
  stemmer = PorterStemmer()
  return [stemmer.stem(w) for w in tokenizedSent]

def myTokenizer(sentence):
  '''
  :params:
  sentence: complete input string

  :returns:
  list of preprocessed individual tokens
  '''
  loweredSent = lowerSent(sentence)
  tokenizedSent = tokenizeSent(loweredSent)
  tokenizedSent = removeStopWords(tokenizedSent)
  stemmedSent = stem(tokenizedSent)
  preprocessedSent = stemmedSent

  return preprocessedSent

def sent_to_words(sentences):
  temp = []
  for sentence in sentences:
    temp.append(myTokenizer(sentence))

  return temp   

data_words = sent_to_words(temp)

print(data_words[5])

tokens = []
for i in data_words:
  for j in i:
    tokens.append(j)
token_count = len(list(set(tokens)))
maxlen = max([len(s) for s in data_words])

def detokenize(text):
  return TreebankWordDetokenizer().detokenize(text)

data = []
for i in data_words:
  data.append(detokenize(i))

data = np.array(data)

labels = np.array(rel_data['target'])
labels = tf.keras.utils.to_categorical(labels, 2, dtype = "float32")

from keras.models import Sequential
from keras import layers
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
max_words = token_count
max_len = maxlen

tokenizer = Tokenizer(num_words = max_words)
tokenizer.fit_on_texts(data)
sequences = tokenizer.texts_to_sequences(data)
tweets = pad_sequences(sequences, maxlen = max_len)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(tweets, labels, random_state = 0)

from keras import regularizers
from keras import backend as K
from keras.callbacks import ModelCheckpoint

model1 = Sequential()
model1.add(layers.Embedding(max_words, 32))
model1.add(layers.LSTM(128, dropout = 0.3))
model1.add(layers.Dense(2, activation = 'softmax'))

model1.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = "accuracy")
checkpoint1 = ModelCheckpoint("best_model1.hdf5", monitor = "accuracy", verbose = 1, save_best_only = True, mode = 'auto', save_freq = 1, save_weights_only = False)
history = model1.fit(X_train, y_train, batch_size = 200, epochs = 75, validation_data = (X_test, y_test), callbacks = [checkpoint1])

best_model = keras.models.load_model("best_model1.hdf5")

import pandas as pd
from google.colab import files
uploaded = files.upload()

test_data = pd.read_csv('test.csv')

import pandas as pd
from google.colab import files
uploaded = files.upload()

sample_submission = pd.read_csv('sample_submission.csv')

temp_list = []
test_data_to_list = test_data['text'].values.tolist()
for i in test_data_to_list:
  temp_list.append(clean_data(i))

test_data_words = sent_to_words(temp_list)

test_tokens = []
for i in test_data_words:
  for j in i:
    test_tokens.append(j)
test_token_count = len(list(set(test_tokens)))
test_maxlen = max([len(s) for s in test_data_words])

test_data = []
for i in test_data_words:
  test_data.append(detokenize(i))

test_data = np.array(test_data)

tokenizer.fit_on_texts(test_data)
test_sequences = tokenizer.texts_to_sequences(test_data)
test_tweets = pad_sequences(test_sequences, maxlen = test_maxlen)
test_tweets[0]

results = best_model.predict(test_tweets)

for i in range(len(results)):
  results[i] = np.around(results[i])

test_sentiments = []
for i in results:
  if i[0] == 1.:
    test_sentiments.append(0)
  else:
    test_sentiments.append(1)

test_data_sentiments = pd.read_csv('test.csv')

test_data_sentiments['sentiment'] = test_sentiments

test_data_sentiments[:10]

sample_submission['target'] = test_sentiments

sample_submission

